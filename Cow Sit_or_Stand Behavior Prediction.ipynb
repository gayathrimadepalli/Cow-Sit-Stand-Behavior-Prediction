{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656b21ef-6be1-4923-9987-68d4e78f1aa1",
   "metadata": {},
   "source": [
    "## Requiremnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502f8b20-d305-46b3-9f09-7538ef5d6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba903e3d-619c-467c-8b8d-9257f10afd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BA74-BB49\n",
      "\n",
      " Directory of C:\\Users\\gayathri.m\\Desktop\n",
      "\n",
      "16-04-2025  10:51    <DIR>          .\n",
      "10-04-2025  10:24    <DIR>          ..\n",
      "14-04-2025  11:45                52 .env\n",
      "16-04-2025  10:27    <DIR>          .ipynb_checkpoints\n",
      "13-02-2025  18:45            47,668 3D_VFX_Pipeline_Documentation.docx\n",
      "25-03-2025  12:50    <DIR>          asset_browser\n",
      "12-02-2025  17:38    <DIR>          asset_manager_sql_test\n",
      "16-04-2025  10:51            65,205 Assignment.ipynb\n",
      "16-04-2025  09:36           931,440 data.csv\n",
      "16-04-2025  09:35            90,711 evaluation_ext.csv\n",
      "16-04-2025  09:35            60,475 ML-Task (7).pdf\n",
      "27-03-2025  10:59             2,368 MongoDBCompass.lnk\n",
      "05-02-2025  10:52    <DIR>          ProductionPipeline\n",
      "04-02-2025  10:48             2,109 Spark.lnk\n",
      "13-02-2025  10:09    <DIR>          sql\n",
      "10-02-2025  17:55    <DIR>          ss\n",
      "14-04-2025  14:49             7,906 td.html\n",
      "04-02-2025  10:15             1,415 Visual Studio Code.lnk\n",
      "              10 File(s)      1,209,349 bytes\n",
      "               8 Dir(s)  813,205,569,536 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb6bac7d-7000-468c-b29a-86e1ad2f634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array_string(s):\n",
    "    if isinstance(s, str):\n",
    "        try:\n",
    "            return np.array(ast.literal_eval(s.replace('\\n', '')))\n",
    "        except:\n",
    "            try:\n",
    "                s = re.sub(r'[\\[\\]]', '', s)\n",
    "                return np.array([float(x) for x in s.split() if x])\n",
    "            except:\n",
    "                return np.array([0.0])  \n",
    "    elif isinstance(s, (int, float)):\n",
    "        return np.array([float(s)])\n",
    "    else:\n",
    "        return np.array([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3ae7c9-06f0-49bc-9d3f-e1ac8d402430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_array_features(arr):\n",
    "    if len(arr) == 0:\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    return [\n",
    "        np.mean(arr),     \n",
    "        np.std(arr),        \n",
    "        np.min(arr),       \n",
    "        np.max(arr),        \n",
    "        np.median(arr),    \n",
    "        np.ptp(arr)         \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c407b1f0-d52f-4f11-b6f5-83e7cc4d5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_array_columns(df, columns):\n",
    "    feature_df = pd.DataFrame()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            parsed_arrays = df[col].apply(parse_array_string)\n",
    "            features = parsed_arrays.apply(extract_array_features)\n",
    "            feature_names = [f\"{col}_mean\", f\"{col}_std\", f\"{col}_min\", f\"{col}_max\", f\"{col}_median\", f\"{col}_range\"]\n",
    "            feature_df[feature_names] = pd.DataFrame(features.tolist(), index=df.index)\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "827ddadd-f366-4dc8-aaad-5f871221d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting useful information ...\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 997 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"extracting useful information ...\")\n",
    "def extract_datetime_features(df):\n",
    "    df[\"datetime\"]=df[\"cdate\"].apply(lambda x:pd.to_datetime(str(x),format='%Y%m%d%H%M'))\n",
    "    df[\"hour\"]=df[\"datetime\"].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669ab36-52b1-4668-a676-e08bf612b90b",
   "metadata": {},
   "source": [
    "## ALL Steps in Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ebde62a-d94f-4333-9c6c-21c0e1a4cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Train shape: (300, 13)\n",
      "Eval shape: (30, 12)\n",
      "   Unnamed: 0   deviceid         cdate  \\\n",
      "0           0  S1I1A1123  202304061110   \n",
      "1           1  S1I1A1124  202304061110   \n",
      "\n",
      "                                                  ax  \\\n",
      "0  [ -5. -32. -35. -33. -34. -38. -37. -34. -38. ...   \n",
      "1  [ -4. -28. -28. -28. -28. -33. -31. -30. -33. ...   \n",
      "\n",
      "                                                  ay  \\\n",
      "0  [ -9. -54. -59. -61. -60. -59. -57. -61. -58. ...   \n",
      "1  [ -9. -58. -62. -61. -61. -62. -62. -60. -60. ...   \n",
      "\n",
      "                                                  az  \\\n",
      "0  [ 3. 17. 20. 19. 19. 20. 21. 19. 20. 19. 19. 2...   \n",
      "1  [ 4. 25. 26. 28. 26. 20. 25. 27. 23. 17. 17. 1...   \n",
      "\n",
      "                                                  gx  \\\n",
      "0  [-11620.  22050.  -5110.  -6720.  14000.   105...   \n",
      "1  [  8610.  -5460.   8540. -12250.  -5250.  1344...   \n",
      "\n",
      "                                                  gy  \\\n",
      "0  [ -7420.  10640.    350.   3360.   1960.  -568...   \n",
      "1  [-14140.  -9800.    140.  15346.   9310. -1407...   \n",
      "\n",
      "                                                  gz  \\\n",
      "0  [ -2030.   -210.  -2240.   4060. -28086.   476...   \n",
      "1  [-19530.  -4130.  -1610.  12110.   1750.  -182...   \n",
      "\n",
      "                                                  mx  \\\n",
      "0  [-163. -169. -136. -157.  -45. -112. -133. -15...   \n",
      "1  [-553. -540. -504. -391. -205. -309. -418. -26...   \n",
      "\n",
      "                                                  my  \\\n",
      "0  [-367. -379. -349. -340. -372. -319. -381. -36...   \n",
      "1  [ -67.  -61.  -63.  -42.    3.  -46.  -66.  -2...   \n",
      "\n",
      "                                                  mz sit_stand  \n",
      "0  [202. 180. 252. 250. 264. 283. 211. 228. 226. ...  Standing  \n",
      "1  [366. 390. 421. 504. 510. 489. 454. 516. 502. ...  Standing  \n",
      "Missing values in train: 0\n",
      "Found 9 array-like columns: ['ax', 'ay', 'az', 'gx', 'gy']\n",
      "Handling array columns...\n",
      "Final train features shape: (300, 59)\n",
      "Train size: 240\n",
      "Val size: 60\n",
      "Fitting basic model...\n",
      "Basic model accuracy: 0.8333\n",
      "Running full pipeline...\n",
      "Best params: {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
      "Evaluating...\n",
      "Validation accuracy: 0.8167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sitting       0.53      0.67      0.59        12\n",
      "    Standing       0.91      0.85      0.88        48\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.72      0.76      0.74        60\n",
      "weighted avg       0.84      0.82      0.82        60\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 8  4]\n",
      " [ 7 41]]\n",
      "Top features:\n",
      "       Feature  Importance\n",
      "58  Unnamed: 0    0.695658\n",
      "54        hour    0.202836\n",
      "57   dayofweek    0.056104\n",
      "55         day    0.045402\n",
      "43      my_std    0.000000\n",
      "32      gz_min    0.000000\n",
      "33      gz_max    0.000000\n",
      "34   gz_median    0.000000\n",
      "35    gz_range    0.000000\n",
      "36     mx_mean    0.000000\n",
      "Predicting on eval data...\n",
      "Predictions saved.\n",
      "Eval prediction distribution:\n",
      "Standing    0.933333\n",
      "Sitting     0.066667\n",
      "Name: proportion, dtype: float64\n",
      "Done.\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    print(\"Loading files...\")\n",
    "    try:\n",
    "        train_data = pd.read_csv('data.csv')\n",
    "        eval_data = pd.read_csv('evaluation_ext.csv')\n",
    "        print(\"Train shape:\", train_data.shape)\n",
    "        print(\"Eval shape:\", eval_data.shape)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Files not found.\")\n",
    "        return\n",
    "\n",
    "    print(train_data.head(2))\n",
    "    print(\"Missing values in train:\", train_data.isnull().sum().sum())\n",
    "\n",
    "    train_data = extract_datetime_features(train_data)\n",
    "    eval_data = extract_datetime_features(eval_data)\n",
    "\n",
    "    array_columns = []\n",
    "    for col in train_data.columns:\n",
    "        if col not in ['deviceid', 'cdate', 'datetime', 'sit_stand', 'hour', 'day', 'month', 'dayofweek']:\n",
    "            val = train_data[col].iloc[0]\n",
    "            if isinstance(val, str) and ('[' in val or '-' in val or '.' in val):\n",
    "                array_columns.append(col)\n",
    "\n",
    "    print(f\"Found {len(array_columns)} array-like columns: {array_columns[:5]}\")\n",
    "\n",
    "    print(\"Handling array columns...\")\n",
    "    train_features = process_array_columns(train_data, array_columns)\n",
    "    eval_features = process_array_columns(eval_data, array_columns)\n",
    "\n",
    "    datetime_cols = ['hour', 'day', 'month', 'dayofweek']\n",
    "    for col in datetime_cols:\n",
    "        train_features[col] = train_data[col]\n",
    "        eval_features[col] = eval_data[col]\n",
    "\n",
    "    for col in train_data.columns:\n",
    "        if col not in ['deviceid', 'cdate', 'datetime', 'sit_stand'] + array_columns + datetime_cols:\n",
    "            try:\n",
    "                train_features[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "                eval_features[col] = pd.to_numeric(eval_data[col], errors='coerce')\n",
    "            except:\n",
    "                print(\"Skipping:\", col)\n",
    "\n",
    "    train_features = train_features.fillna(0)\n",
    "    eval_features = eval_features.fillna(0)\n",
    "\n",
    "    print(\"Final train features shape:\", train_features.shape)\n",
    "\n",
    "    if 'sit_stand' not in train_data.columns:\n",
    "        print(\"Target missing.\")\n",
    "        return\n",
    "\n",
    "    X = train_features\n",
    "    y = train_data['sit_stand']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"Train size:\", X_train.shape[0])\n",
    "    print(\"Val size:\", X_val.shape[0])\n",
    "\n",
    "    print(\"Fitting basic model...\")\n",
    "    basic_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    basic_model.fit(X_train, y_train)\n",
    "    y_pred_basic = basic_model.predict(X_val)\n",
    "    acc_basic = accuracy_score(y_val, y_pred_basic)\n",
    "    print(\"Basic model accuracy:\", round(acc_basic, 4))\n",
    "\n",
    "    print(\"Running full pipeline...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [None, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(\"Best params:\", grid_search.best_params_)\n",
    "    except Exception as e:\n",
    "        print(\"Grid search failed:\", e)\n",
    "        best_model = basic_model\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(\"Validation accuracy:\", round(acc, 4))\n",
    "\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance = best_model.feature_importances_\n",
    "    elif hasattr(best_model, 'named_steps') and hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importance = best_model.named_steps['classifier'].feature_importances_\n",
    "    else:\n",
    "        importance = None\n",
    "\n",
    "    if importance is not None:\n",
    "        fi = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        print(\"Top features:\")\n",
    "        print(fi.head(10))\n",
    "\n",
    "    print(\"Predicting on eval data...\")\n",
    "    eval_preds = best_model.predict(eval_features)\n",
    "    eval_data['sit_stand_predicted'] = eval_preds\n",
    "    eval_data.to_csv('evaluation_with_predictions.csv', index=False)\n",
    "    print(\"Predictions saved.\")\n",
    "\n",
    "    print(\"Eval prediction distribution:\")\n",
    "    print(pd.Series(eval_preds).value_counts(normalize=True))\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae456d1a-172f-4a45-8b57-c92639facc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
